{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GRU, LSTM\n",
    "# GRU and LSTM are RNN it is sequence learning\n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ourIntents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'tag': 'predict', 'patterns': ['How can I kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'tag': 'recommend', 'patterns': ['Recommend c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'tag': 'goodbye', 'patterns': ['bye', 'later'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'tag': 'name', 'patterns': ['what's your name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'tag': 'greeting', 'patterns': ['Hi', 'Hello'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'tag': 'weather', 'patterns': ['What's the we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'tag': 'stores', 'patterns': ['Where are the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'tag': 'crops', 'patterns': ['Where are the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'tag': 'friends', 'patterns': ['I want to mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'tag': 'delete', 'patterns': ['I want to dele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'tag': 'notfound', 'patterns': ['I fail to fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'tag': 'communication', 'patterns': ['I want ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ourIntents\n",
       "0   {'tag': 'predict', 'patterns': ['How can I kno...\n",
       "1   {'tag': 'recommend', 'patterns': ['Recommend c...\n",
       "2   {'tag': 'goodbye', 'patterns': ['bye', 'later'...\n",
       "3   {'tag': 'name', 'patterns': ['what's your name...\n",
       "4   {'tag': 'greeting', 'patterns': ['Hi', 'Hello'...\n",
       "5   {'tag': 'weather', 'patterns': ['What's the we...\n",
       "6   {'tag': 'stores', 'patterns': ['Where are the ...\n",
       "7   {'tag': 'crops', 'patterns': ['Where are the f...\n",
       "8   {'tag': 'friends', 'patterns': ['I want to mak...\n",
       "9   {'tag': 'delete', 'patterns': ['I want to dele...\n",
       "10  {'tag': 'notfound', 'patterns': ['I fail to fi...\n",
       "11  {'tag': 'communication', 'patterns': ['I want ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json(\"intent.json\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for i in range(0,len(data)):\n",
    "    dict[data['ourIntents'][i]['tag']] = cnt;\n",
    "    cnt = cnt+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predict': 0,\n",
       " 'recommend': 1,\n",
       " 'goodbye': 2,\n",
       " 'name': 3,\n",
       " 'greeting': 4,\n",
       " 'weather': 5,\n",
       " 'stores': 6,\n",
       " 'crops': 7,\n",
       " 'friends': 8,\n",
       " 'delete': 9,\n",
       " 'notfound': 10,\n",
       " 'communication': 11}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "y = []\n",
    "for i in range(0,len(data)):\n",
    "    sz = len(data['ourIntents'][i]['patterns'])\n",
    "    for j in range(0,sz):\n",
    "        text.append(data['ourIntents'][i]['patterns'][j])\n",
    "        y.append(data['ourIntents'][i]['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How can I know my yield?',\n",
       " 'Can you predict my yield',\n",
       " 'I want to know my production of crops',\n",
       " 'What is the amount of crops will I get?',\n",
       " 'I want to know production',\n",
       " 'What is production going to be?',\n",
       " 'Recommend crops for me',\n",
       " 'Which crop should I grow?',\n",
       " 'Any idea on which crop is best',\n",
       " 'bye',\n",
       " 'later',\n",
       " 'see you',\n",
       " \"what's your name?\",\n",
       " 'who are you?',\n",
       " 'Hi',\n",
       " 'Hello',\n",
       " 'Hey',\n",
       " \"What's the weather like?\",\n",
       " 'Is it rainy?',\n",
       " 'How should I know my weather?',\n",
       " 'Where are the nearby stores?',\n",
       " 'Stores near me',\n",
       " 'Where should I sell my crops',\n",
       " 'Where are the farmers near me growing?',\n",
       " 'What are the crops grown near me?',\n",
       " 'Crops near me!',\n",
       " 'I want to make new friends',\n",
       " 'I want to talk to my friends',\n",
       " 'Where are my friends?',\n",
       " 'I want to delete my account',\n",
       " 'I dislike you',\n",
       " 'I fail to find crops near me when accessing the major crops',\n",
       " 'I fail to find users near me at major crops feature',\n",
       " \"I can't see users in the major crops\",\n",
       " 'No user is visible in the map',\n",
       " 'I want to communicate with a farmer',\n",
       " 'I want to find fertilizers!!']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [dict[text] for text in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how can i know my yield\n",
      "can you predict my yield\n",
      "i want to know my product of crop\n",
      "what is the amount of crop will i get\n",
      "i want to know product\n",
      "what is product go to be\n",
      "recommend crop for me\n",
      "which crop should i grow\n",
      "ani idea on which crop is best\n",
      "bye\n",
      "later\n",
      "see you\n",
      "what is your name\n",
      "who are you\n",
      "hi\n",
      "hello\n",
      "hey\n",
      "what is the weather like\n",
      "is it raini\n",
      "how should i know my weather\n",
      "where are the nearbi store\n",
      "store near me\n",
      "where should i sell my crop\n",
      "where are the farmer near me grow\n",
      "what are the crop grown near me\n",
      "crop near me\n",
      "i want to make new friend\n",
      "i want to talk to my friend\n",
      "where are my friend\n",
      "i want to delet my account\n",
      "i dislik you\n",
      "i fail to find crop near me when access the major crop\n",
      "i fail to find user near me at major crop featur\n",
      "i can not see user in the major crop\n",
      "no user is visibl in the map\n",
      "i want to commun with a farmer\n",
      "i want to find fertil\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def clean_text(text):\n",
    "    all_reviews = []\n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    for i in range(0,len(text)):\n",
    "        now_text = text[i]\n",
    "        try:\n",
    "            now_text = now_text.lower()\n",
    "            now_text = pattern.sub('', now_text)\n",
    "            now_text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", now_text)\n",
    "            now_text = re.sub(r\"i'm\", \"i am\", now_text)\n",
    "            now_text = re.sub(r\"he's\", \"he is\", now_text)\n",
    "            now_text = re.sub(r\"she's\", \"she is\", now_text)\n",
    "            now_text = re.sub(r\"that's\", \"that is\", now_text)        \n",
    "            now_text = re.sub(r\"what's\", \"what is\", now_text)\n",
    "            now_text = re.sub(r\"where's\", \"where is\", now_text) \n",
    "            now_text = re.sub(r\"\\'ll\", \" will\", now_text)  \n",
    "            now_text = re.sub(r\"\\'ve\", \" have\", now_text)  \n",
    "            now_text = re.sub(r\"\\'re\", \" are\", now_text)\n",
    "            now_text = re.sub(r\"\\'d\", \" would\", now_text)\n",
    "            now_text = re.sub(r\"\\'ve\", \" have\", now_text)\n",
    "            now_text = re.sub(r\"won't\", \"will not\", now_text)\n",
    "            now_text = re.sub(r\"don't\", \"do not\", now_text)\n",
    "            now_text = re.sub(r\"did't\", \"did not\", now_text)\n",
    "            now_text = re.sub(r\"can't\", \"can not\", now_text)\n",
    "            now_text = re.sub(r\"it's\", \"it is\", now_text)\n",
    "            now_text = re.sub(r\"couldn't\", \"could not\", now_text)\n",
    "            now_text = re.sub(r\"have't\", \"have not\", now_text)\n",
    "            tokens = word_tokenize(now_text)\n",
    "            table = str.maketrans('', '', string.punctuation)\n",
    "            stripped = [w.translate(table) for w in tokens]\n",
    "            words = [word for word in stripped if word.isalpha()]\n",
    "            \n",
    "            PS = PorterStemmer()\n",
    "            words = [PS.stem(w) for w in words ]\n",
    "            words = ' '.join(words)\n",
    "            all_reviews.append(words)\n",
    "            print(words)\n",
    "            cnt = cnt+1\n",
    "        except:\n",
    "            \n",
    "            continue\n",
    "    return all_reviews\n",
    "\n",
    "all_reviews = clean_text(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how can i know my yield',\n",
       " 'can you predict my yield',\n",
       " 'i want to know my product of crop',\n",
       " 'what is the amount of crop will i get',\n",
       " 'i want to know product',\n",
       " 'what is product go to be',\n",
       " 'recommend crop for me',\n",
       " 'which crop should i grow',\n",
       " 'ani idea on which crop is best',\n",
       " 'bye',\n",
       " 'later',\n",
       " 'see you',\n",
       " 'what is your name',\n",
       " 'who are you',\n",
       " 'hi',\n",
       " 'hello',\n",
       " 'hey',\n",
       " 'what is the weather like',\n",
       " 'is it raini',\n",
       " 'how should i know my weather',\n",
       " 'where are the nearbi store',\n",
       " 'store near me',\n",
       " 'where should i sell my crop',\n",
       " 'where are the farmer near me grow',\n",
       " 'what are the crop grown near me',\n",
       " 'crop near me',\n",
       " 'i want to make new friend',\n",
       " 'i want to talk to my friend',\n",
       " 'where are my friend',\n",
       " 'i want to delet my account',\n",
       " 'i dislik you',\n",
       " 'i fail to find crop near me when access the major crop',\n",
       " 'i fail to find user near me at major crop featur',\n",
       " 'i can not see user in the major crop',\n",
       " 'no user is visibl in the map',\n",
       " 'i want to commun with a farmer',\n",
       " 'i want to find fertil']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique tokens - 76\n",
      "vocab_size - 77\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "validation_split = 1\n",
    "max_length = 15\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(all_reviews)\n",
    "sequences = tokenizer_obj.texts_to_sequences(all_reviews)\n",
    "\n",
    "word_index = tokenizer_obj.word_index\n",
    "print(\"unique tokens - \"+str(len(word_index)))\n",
    "vocab_size = len(tokenizer_obj.word_index) + 1\n",
    "print('vocab_size - '+str(vocab_size))\n",
    "# This is for the values which are not present in the dataset\n",
    "\n",
    "lines_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "\n",
    "X_train = lines_pad\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(tokenizer_obj,\"tokenizer.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22, 15,  1, 12,  4, 23,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [15, 13, 33,  4, 23,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  6,  3, 12,  4, 16, 24,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [10,  7,  5, 34, 24,  2, 35,  1, 36,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  6,  3, 12, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [10,  7, 16, 37,  3, 38,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [39,  2, 40,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [25,  2, 17,  1, 26,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [41, 42, 43, 25,  2,  7, 44,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [45,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [46,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [27, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [10,  7, 47, 48,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [49, 11, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [50,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [51,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [52,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [10,  7,  5, 28, 53,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 7, 54, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [22, 17,  1, 12,  4, 28,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [14, 11,  5, 56, 29,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [29,  9,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [14, 17,  1, 57,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [14, 11,  5, 30,  9,  8, 26,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [10, 11,  5,  2, 58,  9,  8,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  9,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  6,  3, 59, 60, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  6,  3, 61,  3,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [14, 11,  4, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  6,  3, 62,  4, 63,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1, 64, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1, 31,  3, 19,  2,  9,  8, 65, 66,  5, 20,  2,  0,  0,  0],\n",
       "       [ 1, 31,  3, 19, 21,  9,  8, 67, 20,  2, 68,  0,  0,  0,  0],\n",
       "       [ 1, 15, 69, 27, 21, 32,  5, 20,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [70, 21,  7, 71, 32,  5, 72,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  6,  3, 73, 74, 75, 30,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  6,  3, 19, 76,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 15)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "embedding_dim = 300\n",
    "glove_dir = \"D:\\GloveModel\"\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'), encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coeff = np.asarray(values[1:], dtype='float32')\n",
    "    embedding_index[word] = coeff\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the word  from vector\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m MaxPooling2D,Conv2D,Input,Add,MaxPool2D,Flatten,AveragePooling2D,Dense,BatchNormalization,ZeroPadding2D,Activation,Concatenate,UpSampling2D\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n\u001b[0;32m      4\u001b[0m embedding_layer \u001b[39m=\u001b[39m Embedding(\u001b[39mlen\u001b[39m(word_index) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m      5\u001b[0m                                 embedding_dim,\n\u001b[0;32m      6\u001b[0m                                 weights\u001b[39m=\u001b[39m[embedding_matrix],\n\u001b[0;32m      7\u001b[0m                                 input_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m      8\u001b[0m                                 trainable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_length,\n",
    "                                trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 15, 300)           23100     \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 100)              140400    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 12)                1212      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 164,712\n",
      "Trainable params: 141,612\n",
      "Non-trainable params: 23,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional,GRU\n",
    "\n",
    "\n",
    "model_glove = Sequential()\n",
    "model_glove.add(embedding_layer)\n",
    "model_glove.add(Bidirectional(LSTM(units=50)))\n",
    "model_glove.add(Dense(12, activation='softmax'))\n",
    "\n",
    "model_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model_glove.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 5s 38ms/step - loss: 2.4659 - accuracy: 0.0541\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.3660 - accuracy: 0.2973\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.2938 - accuracy: 0.3243\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 2.2228 - accuracy: 0.3784\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.1564 - accuracy: 0.3784\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.0927 - accuracy: 0.4595\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.0307 - accuracy: 0.4324\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.9792 - accuracy: 0.4054\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 1.9194 - accuracy: 0.4054\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 1.8593 - accuracy: 0.4324\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 1.7971 - accuracy: 0.4595\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.7265 - accuracy: 0.5405\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.6506 - accuracy: 0.6216\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 1.5686 - accuracy: 0.7297\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 1.4903 - accuracy: 0.7297\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 1.4006 - accuracy: 0.7568\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 1.3158 - accuracy: 0.7568\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 1.2436 - accuracy: 0.7297\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 1.1788 - accuracy: 0.7297\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.1095 - accuracy: 0.7297\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 1.0275 - accuracy: 0.7568\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.9501 - accuracy: 0.8108\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.8807 - accuracy: 0.8378\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.8154 - accuracy: 0.8649\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.7459 - accuracy: 0.9459\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6830 - accuracy: 0.9459\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.6260 - accuracy: 0.9459\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.5774 - accuracy: 0.9459\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.5263 - accuracy: 0.9459\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.4736 - accuracy: 0.9730\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.4285 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3937 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3652 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3366 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3082 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2825 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2600 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2392 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2185 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.1992 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1822 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1679 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1554 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1441 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1331 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1230 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1136 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1047 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0975 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0892 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0828 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0763 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0708 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0660 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0611 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0568 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0527 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0495 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0463 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0434 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0402 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0380 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0357 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0342 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0324 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0303 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0290 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0276 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0265 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0254 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0215 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0192 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0101 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1df90515810>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=4)\n",
    "\n",
    "model_glove.fit(X_train, y_train, epochs=100,verbose=1,callbacks = [callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sarcasm(text):\n",
    "    # Send an array of 1 value\n",
    "    test_lines = clean_text(text)\n",
    "    test_sequences = tokenizer_obj.texts_to_sequences(test_lines)\n",
    "    test_review_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "    print(test_review_pad)\n",
    "    pred = model_glove.predict(test_review_pad)\n",
    "    pred*=100\n",
    "    print(pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predict': ['You can go to our homepage and click on get started ðŸ¤'],\n",
       " 'recommend': ['Congrats! You accessed one of to-be-developed features. I am sincerely sorry to tell that the feature is not yet available',\n",
       "  'Features is in development stage. Inconvience regretted'],\n",
       " 'goodbye': ['Bye', 'take care'],\n",
       " 'name': ['My name is farm sensei. Developers named me that',\n",
       "  'I am farm sensei. Ready to be at your service'],\n",
       " 'greeting': ['Hi there. I am Farm Sensei!. Ready to help', 'Hello', 'Hi :)'],\n",
       " 'weather': ['Go to homepage and access weather forecast for our amazing features.'],\n",
       " 'stores': ['Farm sensei to the rescue! You can access stores from the homepage!',\n",
       "  'To know the stores near you can use our stores feature in the homepage'],\n",
       " 'crops': ['Farm sensei to the rescue! You can access crops from the homepage!',\n",
       "  'To know the crops grown near you can use our major crops feature in the homepage ðŸš€'],\n",
       " 'friends': ['Farm sensei to the rescue! Click on your profile on the toppage! There you go you should see a friends bar on the right!!'],\n",
       " 'delete': ['Extremely sorry to hear that! We will improve in the future. If you are unhappy with our service delete the account is present in the profile accessed by clicking on the top of your page'],\n",
       " 'notfound': ['Farm Sensei encourage to fill the form in the profile page. Profile is accessed by clicking on the top of the page. Then you must be able to see the farmers near you!!'],\n",
       " 'communication': ['Farm Sensei encourage to go to profile page. Then you must be able to see the farmers near your area. Send them a friend request. And after accepting you will be able to see their phone number']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary2 = {}\n",
    "for i in range(0,len(dict)):\n",
    "    tag = data[\"ourIntents\"][i]['tag']\n",
    "    sz = len(data[\"ourIntents\"][i]['responses'])\n",
    "    res = []\n",
    "    for j in range(0,sz):\n",
    "        res.append(data['ourIntents'][i]['responses'][j])\n",
    "    dictionary2[tag] = res\n",
    "\n",
    "dictionary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_sarcasm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m text \u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwhat are the crops grown near me\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(TextBlob(text)\u001b[39m.\u001b[39mcorrect())\n\u001b[1;32m----> 5\u001b[0m pred  \u001b[39m=\u001b[39mpredict_sarcasm([text])\n\u001b[0;32m      6\u001b[0m pred[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(pred[\u001b[39m0\u001b[39m])\n\u001b[0;32m      7\u001b[0m i \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(pred[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict_sarcasm' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from textblob import TextBlob\n",
    "text =\"what are the crops grown near me\"\n",
    "text = str(TextBlob(text).correct())\n",
    "pred  =predict_sarcasm([text])\n",
    "pred[0] = np.array(pred[0])\n",
    "i = np.argmax(pred[0])\n",
    "inverse_dict = {value: key for key, value in dict.items()}\n",
    "\n",
    "print(inverse_dict[i])\n",
    "ourResult = random.choice(dictionary2[inverse_dict[i]])\n",
    "print(ourResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Registering two gradient with name 'ReduceDataset'! (Previous registration was in register C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\tensorflow\\\\python\\\\framework\\\\registry.py:65)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# model_glove.save(\"chatbotmodel.h5\")\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m load_model\n\u001b[0;32m      3\u001b[0m \u001b[39m# model_glove = load(\"chatbotmodel.joblib\")\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model_glove \u001b[39m=\u001b[39m load_model(\u001b[39m'\u001b[39m\u001b[39mchatbotmodel.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     41\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\__init__.py:42\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n\u001b[0;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     44\u001b[0m \u001b[39m# from tensorflow.python import keras\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:97\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Experimental API for building input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mThis module contains experimental `Dataset` sources and transformations that can\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39m@@UNKNOWN_CARDINALITY\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m service\n\u001b[0;32m     98\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatching\u001b[39;00m \u001b[39mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[0;32m     99\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatching\u001b[39;00m \u001b[39mimport\u001b[39;00m dense_to_sparse_batch\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:419\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mThis module contains:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m    420\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m from_dataset_id\n\u001b[0;32m    421\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m register_dataset\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservice\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_server_lib\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservice\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_utils\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset_ops\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m options \u001b[39mas\u001b[39;00m options_lib\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m structured_function\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:104\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m# shuffle_op -> dataset_ops).\u001b[39;00m\n\u001b[0;32m     99\u001b[0m shuffle_op \u001b[39m=\u001b[39m lazy_loader\u001b[39m.\u001b[39mLazyLoader(\n\u001b[0;32m    100\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mshuffle_op\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mglobals\u001b[39m(),\n\u001b[0;32m    101\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtensorflow.python.data.ops.shuffle_op\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 104\u001b[0m ops\u001b[39m.\u001b[39;49mNotDifferentiable(\u001b[39m\"\u001b[39;49m\u001b[39mReduceDataset\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    106\u001b[0m \u001b[39m# A constant that can be used to enable auto-tuning.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m AUTOTUNE \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\ops.py:2451\u001b[0m, in \u001b[0;36mno_gradient\u001b[1;34m(op_type)\u001b[0m\n\u001b[0;32m   2449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(op_type, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   2450\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mop_type must be a string\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2451\u001b[0m gradient_registry\u001b[39m.\u001b[39;49mregister(\u001b[39mNone\u001b[39;49;00m, op_type)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\registry.py:57\u001b[0m, in \u001b[0;36mRegistry.register\u001b[1;34m(self, candidate, name)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_registry:\n\u001b[0;32m     56\u001b[0m   frame \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_registry[name][_LOCATION_TAG]\n\u001b[1;32m---> 57\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m     58\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mRegistering two \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m with name \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m! \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m(Previous registration was in \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m     60\u001b[0m       (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, name, frame\u001b[39m.\u001b[39mname, frame\u001b[39m.\u001b[39mfilename, frame\u001b[39m.\u001b[39mlineno))\n\u001b[0;32m     62\u001b[0m logging\u001b[39m.\u001b[39mvlog(\u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRegistering \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) in \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, candidate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name)\n\u001b[0;32m     63\u001b[0m \u001b[39m# stack trace is [this_function, Register(), user_function,...]\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39m# so the user function is #2.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Registering two gradient with name 'ReduceDataset'! (Previous registration was in register C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\tensorflow\\\\python\\\\framework\\\\registry.py:65)\""
     ]
    }
   ],
   "source": [
    "# # model_glove.save(\"chatbotmodel.h5\")\n",
    "# from tensorflow.keras.models import load_model\n",
    "# # model_glove = load(\"chatbotmodel.joblib\")\n",
    "# model_glove = load_model('chatbotmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Registering two gradient with name 'ReduceDataset'! (Previous registration was in register C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\tensorflow\\\\python\\\\framework\\\\registry.py:65)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[0;32m      4\u001b[0m your_model__path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchatbotmodel.keras\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     41\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\__init__.py:42\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n\u001b[0;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     44\u001b[0m \u001b[39m# from tensorflow.python import keras\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:97\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Experimental API for building input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mThis module contains experimental `Dataset` sources and transformations that can\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39m@@UNKNOWN_CARDINALITY\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m service\n\u001b[0;32m     98\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatching\u001b[39;00m \u001b[39mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[0;32m     99\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatching\u001b[39;00m \u001b[39mimport\u001b[39;00m dense_to_sparse_batch\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:419\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mThis module contains:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m    420\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m from_dataset_id\n\u001b[0;32m    421\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m register_dataset\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservice\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_server_lib\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservice\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_utils\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset_ops\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m options \u001b[39mas\u001b[39;00m options_lib\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m structured_function\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:104\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m# shuffle_op -> dataset_ops).\u001b[39;00m\n\u001b[0;32m     99\u001b[0m shuffle_op \u001b[39m=\u001b[39m lazy_loader\u001b[39m.\u001b[39mLazyLoader(\n\u001b[0;32m    100\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mshuffle_op\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mglobals\u001b[39m(),\n\u001b[0;32m    101\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtensorflow.python.data.ops.shuffle_op\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 104\u001b[0m ops\u001b[39m.\u001b[39;49mNotDifferentiable(\u001b[39m\"\u001b[39;49m\u001b[39mReduceDataset\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    106\u001b[0m \u001b[39m# A constant that can be used to enable auto-tuning.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m AUTOTUNE \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\ops.py:2451\u001b[0m, in \u001b[0;36mno_gradient\u001b[1;34m(op_type)\u001b[0m\n\u001b[0;32m   2449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(op_type, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   2450\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mop_type must be a string\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2451\u001b[0m gradient_registry\u001b[39m.\u001b[39;49mregister(\u001b[39mNone\u001b[39;49;00m, op_type)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\registry.py:57\u001b[0m, in \u001b[0;36mRegistry.register\u001b[1;34m(self, candidate, name)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_registry:\n\u001b[0;32m     56\u001b[0m   frame \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_registry[name][_LOCATION_TAG]\n\u001b[1;32m---> 57\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m     58\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mRegistering two \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m with name \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m! \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m(Previous registration was in \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m     60\u001b[0m       (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, name, frame\u001b[39m.\u001b[39mname, frame\u001b[39m.\u001b[39mfilename, frame\u001b[39m.\u001b[39mlineno))\n\u001b[0;32m     62\u001b[0m logging\u001b[39m.\u001b[39mvlog(\u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRegistering \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) in \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, candidate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name)\n\u001b[0;32m     63\u001b[0m \u001b[39m# stack trace is [this_function, Register(), user_function,...]\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39m# so the user function is #2.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Registering two gradient with name 'ReduceDataset'! (Previous registration was in register C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\tensorflow\\\\python\\\\framework\\\\registry.py:65)\""
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
