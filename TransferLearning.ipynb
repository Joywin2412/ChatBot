{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GRU, LSTM\n",
    "# GRU and LSTM are RNN it is sequence learning\n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"Sarcasm_Headlines_Dataset.json\",lines = True)\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['is_sarcastic']\n",
    "df = df.drop(columns=['is_sarcastic','article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline\n",
       "0  former versace store clerk sues over secret 'b...\n",
       "1  the 'roseanne' revival catches up to our thorn...\n",
       "2  mom starting to fear son's web series closest ...\n",
       "3  boehner just wants wife to listen, not come up...\n",
       "4  j.k. rowling wishes snape happy birthday in th..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    1\n",
       "3    1\n",
       "4    0\n",
       "Name: is_sarcastic, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26704</th>\n",
       "      <td>american politics in moral free-fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26705</th>\n",
       "      <td>america's best 20 hikes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26706</th>\n",
       "      <td>reparations and obama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26707</th>\n",
       "      <td>israeli ban targeting boycott supporters raise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26708</th>\n",
       "      <td>gourmet gifts for the foodie 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26709 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline\n",
       "0      former versace store clerk sues over secret 'b...\n",
       "1      the 'roseanne' revival catches up to our thorn...\n",
       "2      mom starting to fear son's web series closest ...\n",
       "3      boehner just wants wife to listen, not come up...\n",
       "4      j.k. rowling wishes snape happy birthday in th...\n",
       "...                                                  ...\n",
       "26704               american politics in moral free-fall\n",
       "26705                            america's best 20 hikes\n",
       "26706                              reparations and obama\n",
       "26707  israeli ban targeting boycott supporters raise...\n",
       "26708                  gourmet gifts for the foodie 2014\n",
       "\n",
       "[26709 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['former versac store clerk sue secret black code minor shopper',\n",
       " 'roseann reviv catch thorni polit mood better wors',\n",
       " 'mom start fear son web seri closest thing grandchild',\n",
       " 'boehner want wife listen not come altern debtreduct idea',\n",
       " 'jk rowl wish snape happi birthday magic way',\n",
       " 'advanc world women',\n",
       " 'fascin case eat labgrown meat',\n",
       " 'ceo send kid school work compani',\n",
       " 'top snake handler leav sink huckabe campaign',\n",
       " 'friday morn email insid trump presser age',\n",
       " 'airlin passeng tackl man rush cockpit bomb threat',\n",
       " 'facebook reportedli work healthcar featur app',\n",
       " 'north korea prais trump urg us voter reject dull hillari',\n",
       " 'actual cnn jeffrey lord indefens',\n",
       " 'barcelona hold huge protest support refuge',\n",
       " 'nuclear bomb deton rehears spiderman music',\n",
       " 'cosbi lawyer ask accus nt come forward smear legal team year ago',\n",
       " 'stock analyst confus frighten boar market',\n",
       " 'bloomberg program build better citi got bigger',\n",
       " 'craig hick indict']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def clean_text(df):\n",
    "    all_reviews = list()\n",
    "    lines = df[\"headline\"].values.tolist()\n",
    "    cnt = 0\n",
    "    \n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    for text in lines:\n",
    "        try:\n",
    "            text = text.lower()\n",
    "            text = pattern.sub('', text)\n",
    "            text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "            tokens = word_tokenize(text)\n",
    "            table = str.maketrans('', '', string.punctuation)\n",
    "            stripped = [w.translate(table) for w in tokens]\n",
    "            words = [word for word in stripped if word.isalpha()]\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "            stop_words.discard(\"not\")\n",
    "            PS = PorterStemmer()\n",
    "            words = [PS.stem(w) for w in words if not w in stop_words]\n",
    "            words = ' '.join(words)\n",
    "            all_reviews.append(words)\n",
    "            cnt = cnt+1\n",
    "        except:\n",
    "            continue\n",
    "    return all_reviews\n",
    "\n",
    "all_reviews = clean_text(df)\n",
    "all_reviews[0:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For xgboost\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CV = CountVectorizer(min_df = 5)   \n",
    "\n",
    "X = CV.fit_transform(all_reviews).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_trainc, X_testc, y_trainc, y_testc = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "CV = TfidfVectorizer(min_df=5)   \n",
    "X = CV.fit_transform(all_reviews).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_traint, X_testt, y_traint, y_testt = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique tokens - 18859\n",
      "vocab_size - 18860\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "validation_split = 0.8\n",
    "max_length = 20\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(all_reviews)\n",
    "sequences = tokenizer_obj.texts_to_sequences(all_reviews)\n",
    "\n",
    "word_index = tokenizer_obj.word_index\n",
    "print(\"unique tokens - \"+str(len(word_index)))\n",
    "vocab_size = len(tokenizer_obj.word_index) + 1\n",
    "print('vocab_size - '+str(vocab_size))\n",
    "# This is for the values which are not present in the dataset\n",
    "\n",
    "lines_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "\n",
    "X_train , X_test ,y_train,y_test = train_test_split(lines_pad,y,train_size=validation_split,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_pad: (21367, 20)\n",
      "Shape of y_train: (21367,)\n",
      "Shape of X_test_pad: (5342, 20)\n",
      "Shape of y_test: (5342,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train_pad:', X_train.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "\n",
    "print('Shape of X_test_pad:', X_test.shape)\n",
    "print('Shape of y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a GloVE model to obtain the vector representation of the words of a pretrained model and then get the mapping from words to\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "embedding_dim = 300\n",
    "glove_dir = \"D:\\GloveModel\"\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'), encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coeff = np.asarray(values[1:], dtype='float32')\n",
    "    embedding_index[word] = coeff\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the word  from vector\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 20, 300)           5658000   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 64)               85248     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,743,313\n",
      "Trainable params: 85,313\n",
      "Non-trainable params: 5,658,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional,GRU\n",
    "\n",
    "\n",
    "model_glove = Sequential()\n",
    "model_glove.add(embedding_layer)\n",
    "model_glove.add(Bidirectional(LSTM(units=32, recurrent_dropout = 0.4, dropout = 0.4)))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model_glove.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668/668 [==============================] - 19s 28ms/step - loss: 0.3151 - accuracy: 0.8625 - val_loss: 0.4528 - val_accuracy: 0.8008\n",
      "Epoch 2/100\n",
      "599/668 [=========================>....] - ETA: 1s - loss: 0.3076 - accuracy: 0.8645"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m callback \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model_glove\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test, y_test), verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,callbacks \u001b[39m=\u001b[39;49m [callback])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=4)\n",
    "\n",
    "model_glove.fit(X_train, y_train, epochs=100,batch_size=32, validation_data=(X_test, y_test), verbose=1,callbacks = [callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 1s 4ms/step\n",
      "Accuracy: 0.8013852489704231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model_glove.predict(X_test)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.66346\tvalidation_1-logloss:0.66543\n",
      "[1]\tvalidation_0-logloss:0.64776\tvalidation_1-logloss:0.65100\n",
      "[2]\tvalidation_0-logloss:0.63789\tvalidation_1-logloss:0.64083\n",
      "[3]\tvalidation_0-logloss:0.63004\tvalidation_1-logloss:0.63360\n",
      "[4]\tvalidation_0-logloss:0.62497\tvalidation_1-logloss:0.62954\n",
      "[5]\tvalidation_0-logloss:0.62057\tvalidation_1-logloss:0.62539\n",
      "[6]\tvalidation_0-logloss:0.61642\tvalidation_1-logloss:0.62099\n",
      "[7]\tvalidation_0-logloss:0.61313\tvalidation_1-logloss:0.61764\n",
      "[8]\tvalidation_0-logloss:0.60974\tvalidation_1-logloss:0.61536\n",
      "[9]\tvalidation_0-logloss:0.60715\tvalidation_1-logloss:0.61276\n",
      "[10]\tvalidation_0-logloss:0.60446\tvalidation_1-logloss:0.61095\n",
      "[11]\tvalidation_0-logloss:0.60120\tvalidation_1-logloss:0.60783\n",
      "[12]\tvalidation_0-logloss:0.59863\tvalidation_1-logloss:0.60504\n",
      "[13]\tvalidation_0-logloss:0.59638\tvalidation_1-logloss:0.60304\n",
      "[14]\tvalidation_0-logloss:0.59414\tvalidation_1-logloss:0.60073\n",
      "[15]\tvalidation_0-logloss:0.59200\tvalidation_1-logloss:0.59861\n",
      "[16]\tvalidation_0-logloss:0.59019\tvalidation_1-logloss:0.59683\n",
      "[17]\tvalidation_0-logloss:0.58802\tvalidation_1-logloss:0.59538\n",
      "[18]\tvalidation_0-logloss:0.58607\tvalidation_1-logloss:0.59313\n",
      "[19]\tvalidation_0-logloss:0.58441\tvalidation_1-logloss:0.59177\n",
      "[20]\tvalidation_0-logloss:0.58258\tvalidation_1-logloss:0.59045\n",
      "[21]\tvalidation_0-logloss:0.58096\tvalidation_1-logloss:0.58894\n",
      "[22]\tvalidation_0-logloss:0.57944\tvalidation_1-logloss:0.58753\n",
      "[23]\tvalidation_0-logloss:0.57786\tvalidation_1-logloss:0.58632\n",
      "[24]\tvalidation_0-logloss:0.57639\tvalidation_1-logloss:0.58499\n",
      "[25]\tvalidation_0-logloss:0.57476\tvalidation_1-logloss:0.58332\n",
      "[26]\tvalidation_0-logloss:0.57345\tvalidation_1-logloss:0.58239\n",
      "[27]\tvalidation_0-logloss:0.57219\tvalidation_1-logloss:0.58135\n",
      "[28]\tvalidation_0-logloss:0.57039\tvalidation_1-logloss:0.58026\n",
      "[29]\tvalidation_0-logloss:0.56912\tvalidation_1-logloss:0.57934\n",
      "[30]\tvalidation_0-logloss:0.56792\tvalidation_1-logloss:0.57853\n",
      "[31]\tvalidation_0-logloss:0.56657\tvalidation_1-logloss:0.57746\n",
      "[32]\tvalidation_0-logloss:0.56521\tvalidation_1-logloss:0.57634\n",
      "[33]\tvalidation_0-logloss:0.56401\tvalidation_1-logloss:0.57544\n",
      "[34]\tvalidation_0-logloss:0.56290\tvalidation_1-logloss:0.57524\n",
      "[35]\tvalidation_0-logloss:0.56175\tvalidation_1-logloss:0.57407\n",
      "[36]\tvalidation_0-logloss:0.56036\tvalidation_1-logloss:0.57326\n",
      "[37]\tvalidation_0-logloss:0.55909\tvalidation_1-logloss:0.57194\n",
      "[38]\tvalidation_0-logloss:0.55797\tvalidation_1-logloss:0.57111\n",
      "[39]\tvalidation_0-logloss:0.55696\tvalidation_1-logloss:0.57031\n",
      "[40]\tvalidation_0-logloss:0.55567\tvalidation_1-logloss:0.56955\n",
      "[41]\tvalidation_0-logloss:0.55464\tvalidation_1-logloss:0.56865\n",
      "[42]\tvalidation_0-logloss:0.55364\tvalidation_1-logloss:0.56808\n",
      "[43]\tvalidation_0-logloss:0.55251\tvalidation_1-logloss:0.56688\n",
      "[44]\tvalidation_0-logloss:0.55144\tvalidation_1-logloss:0.56599\n",
      "[45]\tvalidation_0-logloss:0.55041\tvalidation_1-logloss:0.56552\n",
      "[46]\tvalidation_0-logloss:0.54947\tvalidation_1-logloss:0.56510\n",
      "[47]\tvalidation_0-logloss:0.54857\tvalidation_1-logloss:0.56451\n",
      "[48]\tvalidation_0-logloss:0.54728\tvalidation_1-logloss:0.56372\n",
      "[49]\tvalidation_0-logloss:0.54629\tvalidation_1-logloss:0.56298\n",
      "[50]\tvalidation_0-logloss:0.54544\tvalidation_1-logloss:0.56230\n",
      "[51]\tvalidation_0-logloss:0.54455\tvalidation_1-logloss:0.56149\n",
      "[52]\tvalidation_0-logloss:0.54363\tvalidation_1-logloss:0.56042\n",
      "[53]\tvalidation_0-logloss:0.54282\tvalidation_1-logloss:0.55978\n",
      "[54]\tvalidation_0-logloss:0.54201\tvalidation_1-logloss:0.55914\n",
      "[55]\tvalidation_0-logloss:0.54104\tvalidation_1-logloss:0.55845\n",
      "[56]\tvalidation_0-logloss:0.54016\tvalidation_1-logloss:0.55766\n",
      "[57]\tvalidation_0-logloss:0.53911\tvalidation_1-logloss:0.55694\n",
      "[58]\tvalidation_0-logloss:0.53831\tvalidation_1-logloss:0.55643\n",
      "[59]\tvalidation_0-logloss:0.53746\tvalidation_1-logloss:0.55570\n",
      "[60]\tvalidation_0-logloss:0.53654\tvalidation_1-logloss:0.55512\n",
      "[61]\tvalidation_0-logloss:0.53566\tvalidation_1-logloss:0.55427\n",
      "[62]\tvalidation_0-logloss:0.53492\tvalidation_1-logloss:0.55356\n",
      "[63]\tvalidation_0-logloss:0.53399\tvalidation_1-logloss:0.55292\n",
      "[64]\tvalidation_0-logloss:0.53327\tvalidation_1-logloss:0.55265\n",
      "[65]\tvalidation_0-logloss:0.53248\tvalidation_1-logloss:0.55180\n",
      "[66]\tvalidation_0-logloss:0.53152\tvalidation_1-logloss:0.55083\n",
      "[67]\tvalidation_0-logloss:0.53052\tvalidation_1-logloss:0.55057\n",
      "[68]\tvalidation_0-logloss:0.52958\tvalidation_1-logloss:0.54996\n",
      "[69]\tvalidation_0-logloss:0.52886\tvalidation_1-logloss:0.54938\n",
      "[70]\tvalidation_0-logloss:0.52817\tvalidation_1-logloss:0.54910\n",
      "[71]\tvalidation_0-logloss:0.52747\tvalidation_1-logloss:0.54832\n",
      "[72]\tvalidation_0-logloss:0.52677\tvalidation_1-logloss:0.54767\n",
      "[73]\tvalidation_0-logloss:0.52590\tvalidation_1-logloss:0.54673\n",
      "[74]\tvalidation_0-logloss:0.52516\tvalidation_1-logloss:0.54620\n",
      "[75]\tvalidation_0-logloss:0.52452\tvalidation_1-logloss:0.54558\n",
      "[76]\tvalidation_0-logloss:0.52376\tvalidation_1-logloss:0.54471\n",
      "[77]\tvalidation_0-logloss:0.52312\tvalidation_1-logloss:0.54449\n",
      "[78]\tvalidation_0-logloss:0.52244\tvalidation_1-logloss:0.54431\n",
      "[79]\tvalidation_0-logloss:0.52153\tvalidation_1-logloss:0.54402\n",
      "[80]\tvalidation_0-logloss:0.52089\tvalidation_1-logloss:0.54323\n",
      "[81]\tvalidation_0-logloss:0.52005\tvalidation_1-logloss:0.54292\n",
      "[82]\tvalidation_0-logloss:0.51933\tvalidation_1-logloss:0.54242\n",
      "[83]\tvalidation_0-logloss:0.51850\tvalidation_1-logloss:0.54180\n",
      "[84]\tvalidation_0-logloss:0.51781\tvalidation_1-logloss:0.54128\n",
      "[85]\tvalidation_0-logloss:0.51704\tvalidation_1-logloss:0.54098\n",
      "[86]\tvalidation_0-logloss:0.51631\tvalidation_1-logloss:0.53998\n",
      "[87]\tvalidation_0-logloss:0.51566\tvalidation_1-logloss:0.53929\n",
      "[88]\tvalidation_0-logloss:0.51509\tvalidation_1-logloss:0.53894\n",
      "[89]\tvalidation_0-logloss:0.51451\tvalidation_1-logloss:0.53877\n",
      "[90]\tvalidation_0-logloss:0.51394\tvalidation_1-logloss:0.53818\n",
      "[91]\tvalidation_0-logloss:0.51330\tvalidation_1-logloss:0.53762\n",
      "[92]\tvalidation_0-logloss:0.51276\tvalidation_1-logloss:0.53708\n",
      "[93]\tvalidation_0-logloss:0.51221\tvalidation_1-logloss:0.53709\n",
      "[94]\tvalidation_0-logloss:0.51163\tvalidation_1-logloss:0.53689\n",
      "[95]\tvalidation_0-logloss:0.51107\tvalidation_1-logloss:0.53674\n",
      "[96]\tvalidation_0-logloss:0.51055\tvalidation_1-logloss:0.53628\n",
      "[97]\tvalidation_0-logloss:0.50992\tvalidation_1-logloss:0.53599\n",
      "[98]\tvalidation_0-logloss:0.50928\tvalidation_1-logloss:0.53582\n",
      "[99]\tvalidation_0-logloss:0.50873\tvalidation_1-logloss:0.53569\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_trainc,y_trainc , eval_set = [(X_trainc,y_trainc),(X_testc,y_testc)],early_stopping_rounds=10)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7283788843129914\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_testc)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.66298\tvalidation_1-logloss:0.66517\n",
      "[1]\tvalidation_0-logloss:0.64681\tvalidation_1-logloss:0.65042\n",
      "[2]\tvalidation_0-logloss:0.63605\tvalidation_1-logloss:0.64031\n",
      "[3]\tvalidation_0-logloss:0.62802\tvalidation_1-logloss:0.63267\n",
      "[4]\tvalidation_0-logloss:0.62195\tvalidation_1-logloss:0.62769\n",
      "[5]\tvalidation_0-logloss:0.61759\tvalidation_1-logloss:0.62450\n",
      "[6]\tvalidation_0-logloss:0.61323\tvalidation_1-logloss:0.62137\n",
      "[7]\tvalidation_0-logloss:0.60965\tvalidation_1-logloss:0.61774\n",
      "[8]\tvalidation_0-logloss:0.60620\tvalidation_1-logloss:0.61448\n",
      "[9]\tvalidation_0-logloss:0.60307\tvalidation_1-logloss:0.61184\n",
      "[10]\tvalidation_0-logloss:0.60016\tvalidation_1-logloss:0.60958\n",
      "[11]\tvalidation_0-logloss:0.59760\tvalidation_1-logloss:0.60794\n",
      "[12]\tvalidation_0-logloss:0.59502\tvalidation_1-logloss:0.60557\n",
      "[13]\tvalidation_0-logloss:0.59221\tvalidation_1-logloss:0.60309\n",
      "[14]\tvalidation_0-logloss:0.58959\tvalidation_1-logloss:0.60068\n",
      "[15]\tvalidation_0-logloss:0.58743\tvalidation_1-logloss:0.59853\n",
      "[16]\tvalidation_0-logloss:0.58531\tvalidation_1-logloss:0.59646\n",
      "[17]\tvalidation_0-logloss:0.58306\tvalidation_1-logloss:0.59456\n",
      "[18]\tvalidation_0-logloss:0.58052\tvalidation_1-logloss:0.59320\n",
      "[19]\tvalidation_0-logloss:0.57830\tvalidation_1-logloss:0.59176\n",
      "[20]\tvalidation_0-logloss:0.57632\tvalidation_1-logloss:0.59058\n",
      "[21]\tvalidation_0-logloss:0.57440\tvalidation_1-logloss:0.58884\n",
      "[22]\tvalidation_0-logloss:0.57243\tvalidation_1-logloss:0.58729\n",
      "[23]\tvalidation_0-logloss:0.57060\tvalidation_1-logloss:0.58566\n",
      "[24]\tvalidation_0-logloss:0.56904\tvalidation_1-logloss:0.58367\n",
      "[25]\tvalidation_0-logloss:0.56732\tvalidation_1-logloss:0.58201\n",
      "[26]\tvalidation_0-logloss:0.56578\tvalidation_1-logloss:0.58120\n",
      "[27]\tvalidation_0-logloss:0.56391\tvalidation_1-logloss:0.58025\n",
      "[28]\tvalidation_0-logloss:0.56243\tvalidation_1-logloss:0.57965\n",
      "[29]\tvalidation_0-logloss:0.56107\tvalidation_1-logloss:0.57855\n",
      "[30]\tvalidation_0-logloss:0.55947\tvalidation_1-logloss:0.57799\n",
      "[31]\tvalidation_0-logloss:0.55786\tvalidation_1-logloss:0.57738\n",
      "[32]\tvalidation_0-logloss:0.55649\tvalidation_1-logloss:0.57637\n",
      "[33]\tvalidation_0-logloss:0.55507\tvalidation_1-logloss:0.57559\n",
      "[34]\tvalidation_0-logloss:0.55347\tvalidation_1-logloss:0.57477\n",
      "[35]\tvalidation_0-logloss:0.55216\tvalidation_1-logloss:0.57359\n",
      "[36]\tvalidation_0-logloss:0.55098\tvalidation_1-logloss:0.57231\n",
      "[37]\tvalidation_0-logloss:0.54975\tvalidation_1-logloss:0.57191\n",
      "[38]\tvalidation_0-logloss:0.54852\tvalidation_1-logloss:0.57125\n",
      "[39]\tvalidation_0-logloss:0.54739\tvalidation_1-logloss:0.57052\n",
      "[40]\tvalidation_0-logloss:0.54601\tvalidation_1-logloss:0.56936\n",
      "[41]\tvalidation_0-logloss:0.54444\tvalidation_1-logloss:0.56839\n",
      "[42]\tvalidation_0-logloss:0.54330\tvalidation_1-logloss:0.56751\n",
      "[43]\tvalidation_0-logloss:0.54215\tvalidation_1-logloss:0.56605\n",
      "[44]\tvalidation_0-logloss:0.54102\tvalidation_1-logloss:0.56493\n",
      "[45]\tvalidation_0-logloss:0.53968\tvalidation_1-logloss:0.56440\n",
      "[46]\tvalidation_0-logloss:0.53803\tvalidation_1-logloss:0.56341\n",
      "[47]\tvalidation_0-logloss:0.53685\tvalidation_1-logloss:0.56314\n",
      "[48]\tvalidation_0-logloss:0.53586\tvalidation_1-logloss:0.56267\n",
      "[49]\tvalidation_0-logloss:0.53485\tvalidation_1-logloss:0.56180\n",
      "[50]\tvalidation_0-logloss:0.53382\tvalidation_1-logloss:0.56142\n",
      "[51]\tvalidation_0-logloss:0.53264\tvalidation_1-logloss:0.56080\n",
      "[52]\tvalidation_0-logloss:0.53165\tvalidation_1-logloss:0.56004\n",
      "[53]\tvalidation_0-logloss:0.53052\tvalidation_1-logloss:0.55947\n",
      "[54]\tvalidation_0-logloss:0.52942\tvalidation_1-logloss:0.55874\n",
      "[55]\tvalidation_0-logloss:0.52828\tvalidation_1-logloss:0.55846\n",
      "[56]\tvalidation_0-logloss:0.52728\tvalidation_1-logloss:0.55804\n",
      "[57]\tvalidation_0-logloss:0.52612\tvalidation_1-logloss:0.55712\n",
      "[58]\tvalidation_0-logloss:0.52519\tvalidation_1-logloss:0.55653\n",
      "[59]\tvalidation_0-logloss:0.52426\tvalidation_1-logloss:0.55582\n",
      "[60]\tvalidation_0-logloss:0.52341\tvalidation_1-logloss:0.55540\n",
      "[61]\tvalidation_0-logloss:0.52250\tvalidation_1-logloss:0.55458\n",
      "[62]\tvalidation_0-logloss:0.52166\tvalidation_1-logloss:0.55398\n",
      "[63]\tvalidation_0-logloss:0.52062\tvalidation_1-logloss:0.55320\n",
      "[64]\tvalidation_0-logloss:0.51967\tvalidation_1-logloss:0.55270\n",
      "[65]\tvalidation_0-logloss:0.51860\tvalidation_1-logloss:0.55197\n",
      "[66]\tvalidation_0-logloss:0.51761\tvalidation_1-logloss:0.55130\n",
      "[67]\tvalidation_0-logloss:0.51671\tvalidation_1-logloss:0.55074\n",
      "[68]\tvalidation_0-logloss:0.51546\tvalidation_1-logloss:0.55042\n",
      "[69]\tvalidation_0-logloss:0.51469\tvalidation_1-logloss:0.55013\n",
      "[70]\tvalidation_0-logloss:0.51371\tvalidation_1-logloss:0.54928\n",
      "[71]\tvalidation_0-logloss:0.51295\tvalidation_1-logloss:0.54890\n",
      "[72]\tvalidation_0-logloss:0.51192\tvalidation_1-logloss:0.54853\n",
      "[73]\tvalidation_0-logloss:0.51094\tvalidation_1-logloss:0.54801\n",
      "[74]\tvalidation_0-logloss:0.51001\tvalidation_1-logloss:0.54747\n",
      "[75]\tvalidation_0-logloss:0.50921\tvalidation_1-logloss:0.54677\n",
      "[76]\tvalidation_0-logloss:0.50830\tvalidation_1-logloss:0.54673\n",
      "[77]\tvalidation_0-logloss:0.50751\tvalidation_1-logloss:0.54647\n",
      "[78]\tvalidation_0-logloss:0.50677\tvalidation_1-logloss:0.54613\n",
      "[79]\tvalidation_0-logloss:0.50584\tvalidation_1-logloss:0.54553\n",
      "[80]\tvalidation_0-logloss:0.50484\tvalidation_1-logloss:0.54479\n",
      "[81]\tvalidation_0-logloss:0.50399\tvalidation_1-logloss:0.54435\n",
      "[82]\tvalidation_0-logloss:0.50320\tvalidation_1-logloss:0.54406\n",
      "[83]\tvalidation_0-logloss:0.50221\tvalidation_1-logloss:0.54327\n",
      "[84]\tvalidation_0-logloss:0.50142\tvalidation_1-logloss:0.54284\n",
      "[85]\tvalidation_0-logloss:0.50061\tvalidation_1-logloss:0.54239\n",
      "[86]\tvalidation_0-logloss:0.49995\tvalidation_1-logloss:0.54204\n",
      "[87]\tvalidation_0-logloss:0.49905\tvalidation_1-logloss:0.54134\n",
      "[88]\tvalidation_0-logloss:0.49800\tvalidation_1-logloss:0.54089\n",
      "[89]\tvalidation_0-logloss:0.49722\tvalidation_1-logloss:0.54040\n",
      "[90]\tvalidation_0-logloss:0.49649\tvalidation_1-logloss:0.54015\n",
      "[91]\tvalidation_0-logloss:0.49574\tvalidation_1-logloss:0.54007\n",
      "[92]\tvalidation_0-logloss:0.49475\tvalidation_1-logloss:0.53943\n",
      "[93]\tvalidation_0-logloss:0.49411\tvalidation_1-logloss:0.53915\n",
      "[94]\tvalidation_0-logloss:0.49327\tvalidation_1-logloss:0.53841\n",
      "[95]\tvalidation_0-logloss:0.49243\tvalidation_1-logloss:0.53811\n",
      "[96]\tvalidation_0-logloss:0.49164\tvalidation_1-logloss:0.53771\n",
      "[97]\tvalidation_0-logloss:0.49086\tvalidation_1-logloss:0.53744\n",
      "[98]\tvalidation_0-logloss:0.49011\tvalidation_1-logloss:0.53695\n",
      "[99]\tvalidation_0-logloss:0.48919\tvalidation_1-logloss:0.53645\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_traint,y_traint , eval_set = [(X_traint,y_traint),(X_testt,y_testt)],early_stopping_rounds=10)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_testt)\n\u001b[0;32m      2\u001b[0m y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(y_pred \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m accuracy \u001b[39m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_testt)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.675215275177836\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "model = GaussianNB()\n",
    "model.fit(X_trainc,y_trainc)\n",
    "\n",
    "y_pred = model.predict(X_testc)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "print(model.score(X_testc,y_testc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7869711718457506\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "model = BernoulliNB()\n",
    "model.fit(X_trainc,y_trainc)\n",
    "\n",
    "y_pred = model.predict(X_testt)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "print(model.score(X_testt,y_testt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNmodelsarcasm.joblib']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from joblib import dump\n",
    "# dump(model_glove,\"NNmodelsarcasm.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x2216499bd10>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import load\n",
    "load(\"NNmodelsarcasm.joblib\",model_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNtestemodelsarcasm.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from joblib import dump\n",
    "# dump(model_glove,\"NNtestemodelsarcasm.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sarcasm(s):\n",
    "    x_final = pd.DataFrame({\"headline\":[s]})\n",
    "    test_lines = clean_text(x_final)\n",
    "        test_sequences = tokenizer_obj.texts_to_sequences(test_lines)\n",
    "        test_review_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "    pred = model_glove.predict(test_review_pad)\n",
    "    pred*=100\n",
    "    if pred[0][0]>=50: return \"It's a sarcasm!\" \n",
    "    else: return \"It's not a sarcasm.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 183ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's not a sarcasm.\""
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"I was depressed. He asked me to be happy. I am not depressed anymore.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's a sarcasm!\""
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"You just broke my car window. Great job.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's not a sarcasm.\""
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"You just saved my dog's life. Thanks a million.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's not a sarcasm.\""
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"I want a million dollars!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's a sarcasm!\""
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"I just won a million dollars!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's a sarcasm!\""
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"His name is Bob. He is a nice person.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's a sarcasm!\""
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"Sarcasm is very easy to detect.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's not a sarcasm.\""
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"That's just what I needed today!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's not a sarcasm.\""
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sarcasm(\"I work 40 hours a week for me to be this poor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
